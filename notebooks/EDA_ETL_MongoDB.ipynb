{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA y ETL MongoDB - Replicaci√≥n Primario-Secundario\n",
    "## Dataset: Brazilian E-commerce (Kaggle)\n",
    "\n",
    "Este notebook realiza:\n",
    "1. **Descarga autom√°tica** del dataset desde Kaggle usando kagglehub\n",
    "2. **An√°lisis Exploratorio de Datos (EDA)** de los archivos CSV\n",
    "3. **Extracci√≥n, Transformaci√≥n y Carga (ETL)**\n",
    "4. **Carga a MongoDB** con verificaci√≥n de replicaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librer√≠as necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from pymongo import MongoClient\n",
    "import warnings\n",
    "import os\n",
    "import kagglehub\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar estilo de gr√°ficos\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Descarga del Dataset desde Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargar dataset usando kagglehub\n",
    "print(\"üì• Descargando dataset de Brazilian E-commerce...\")\n",
    "\n",
    "try:\n",
    "    # Descargar el dataset\n",
    "    path = kagglehub.dataset_download(\"olistbr/brazilian-ecommerce\")\n",
    "    print(f\"‚úÖ Dataset descargado en: {path}\")\n",
    "    \n",
    "    # Listar archivos descargados\n",
    "    files = list(Path(path).glob(\"*.csv\"))\n",
    "    print(f\"\\nüìÅ Archivos CSV encontrados ({len(files)}):\")\n",
    "    for file in files:\n",
    "        print(f\"  - {file.name}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error al descargar: {e}\")\n",
    "    print(\"üí° Aseg√∫rate de tener kagglehub instalado: pip install kagglehub\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carga y Exploraci√≥n de los Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar todos los archivos CSV\n",
    "print(\"üìä Cargando archivos CSV...\")\n",
    "\n",
    "dataframes = {}\n",
    "\n",
    "for file in files:\n",
    "    df_name = file.stem  # Nombre del archivo sin extensi√≥n\n",
    "    print(f\"\\nüìñ Cargando {file.name}...\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        dataframes[df_name] = df\n",
    "        print(f\"  ‚úÖ Filas: {len(df)}, Columnas: {len(df.columns)}\")\n",
    "        print(f\"  üìã Columnas: {list(df.columns)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error al cargar {file.name}: {e}\")\n",
    "\n",
    "print(f\"\\nüéâ Total de datasets cargados: {len(dataframes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. An√°lisis Exploratorio de Datos (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar informaci√≥n b√°sica de cada dataset\n",
    "print(\"üîç AN√ÅLISIS EXPLORATORIO DE DATOS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"\\nüìä DATASET: {name.upper()}\")\n",
    "    print(f\"Dimensiones: {df.shape}\")\n",
    "    print(f\"\\nPrimeras 3 filas:\")\n",
    "    display(df.head(3))\n",
    "    \n",
    "    print(f\"\\nInformaci√≥n del dataset:\")\n",
    "    print(df.info())\n",
    "    \n",
    "    print(f\"\\nValores nulos:\")\n",
    "    null_counts = df.isnull().sum()\n",
    "    if null_counts.sum() > 0:\n",
    "        print(null_counts[null_counts > 0])\n",
    "    else:\n",
    "        print(\"‚úÖ No hay valores nulos\")\n",
    "    \n",
    "    print(f\"\\nEstad√≠sticas descriptivas:\")\n",
    "    display(df.describe())\n",
    "    \n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Limpieza y Transformaci√≥n de Datos (ETL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ PROCESO ETL - COMBINANDO DATASETS\n",
      "==================================================\n",
      "üì¶ Orders: (99441, 8)\n",
      "üì¶ Items: (112650, 7)\n",
      "üì¶ Products: (32951, 9)\n",
      "üì¶ Customers: (99441, 5)\n",
      "üì¶ Sellers: (3095, 4)\n"
     ]
    }
   ],
   "source": [
    "# ETL: Combinar datasets para crear un dataset unificado de ventas\n",
    "print(\"üîÑ PROCESO ETL - COMBINANDO DATASETS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Obtener los datasets principales\n",
    "orders_df = dataframes.get('olist_orders_dataset', pd.DataFrame())\n",
    "items_df = dataframes.get('olist_order_items_dataset', pd.DataFrame())\n",
    "products_df = dataframes.get('olist_products_dataset', pd.DataFrame())\n",
    "customers_df = dataframes.get('olist_customers_dataset', pd.DataFrame())\n",
    "sellers_df = dataframes.get('olist_sellers_dataset', pd.DataFrame())\n",
    "\n",
    "print(f\"üì¶ Orders: {orders_df.shape}\")\n",
    "print(f\"üì¶ Items: {items_df.shape}\")\n",
    "print(f\"üì¶ Products: {products_df.shape}\")\n",
    "print(f\"üì¶ Customers: {customers_df.shape}\")\n",
    "print(f\"üì¶ Sellers: {sellers_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üïí Transformando fechas...\n",
      "‚úÖ Fechas transformadas\n"
     ]
    }
   ],
   "source": [
    "# Limpiar y transformar fechas\n",
    "print(\"\\nüïí Transformando fechas...\")\n",
    "\n",
    "if not orders_df.empty:\n",
    "    # Convertir columnas de fecha\n",
    "    date_columns = ['order_purchase_date', 'order_approved_at', 'order_delivered_carrier_date', \n",
    "                   'order_delivered_customer_date', 'order_estimated_delivery_date']\n",
    "    \n",
    "    for col in date_columns:\n",
    "        if col in orders_df.columns:\n",
    "            orders_df[col] = pd.to_datetime(orders_df[col], errors='coerce')\n",
    "    \n",
    "    print(\"‚úÖ Fechas transformadas\")\n",
    "    \n",
    "    # Mostrar rango de fechas\n",
    "    if 'order_purchase_date' in orders_df.columns:\n",
    "        print(f\"üìÖ Rango de fechas de compra: {orders_df['order_purchase_date'].min()} a {orders_df['order_purchase_date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó Combinando datasets...\n",
      "‚úÖ Orders + Items: (112650, 14)\n",
      "‚úÖ + Products: (112650, 22)\n",
      "‚úÖ + Customers: (112650, 26)\n",
      "‚úÖ + Sellers: (112650, 29)\n",
      "\n",
      "üéâ Dataset combinado final: (112650, 29)\n"
     ]
    }
   ],
   "source": [
    "# Combinar datasets\n",
    "print(\"\\nüîó Combinando datasets...\")\n",
    "\n",
    "try:\n",
    "    # Merge 1: Orders + Items\n",
    "    if not orders_df.empty and not items_df.empty:\n",
    "        ventas_df = orders_df.merge(items_df, on='order_id', how='inner')\n",
    "        print(f\"‚úÖ Orders + Items: {ventas_df.shape}\")\n",
    "    \n",
    "    # Merge 2: + Products\n",
    "    if not products_df.empty:\n",
    "        ventas_df = ventas_df.merge(products_df, on='product_id', how='left')\n",
    "        print(f\"‚úÖ + Products: {ventas_df.shape}\")\n",
    "    \n",
    "    # Merge 3: + Customers\n",
    "    if not customers_df.empty:\n",
    "        ventas_df = ventas_df.merge(customers_df, on='customer_id', how='left')\n",
    "        print(f\"‚úÖ + Customers: {ventas_df.shape}\")\n",
    "    \n",
    "    # Merge 4: + Sellers\n",
    "    if not sellers_df.empty:\n",
    "        ventas_df = ventas_df.merge(sellers_df, on='seller_id', how='left')\n",
    "        print(f\"‚úÖ + Sellers: {ventas_df.shape}\")\n",
    "    \n",
    "    print(f\"\\nüéâ Dataset combinado final: {ventas_df.shape}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error al combinar: {e}\")\n",
    "    # Crear dataset de ejemplo si falla la combinaci√≥n\n",
    "    ventas_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üßπ Limpiando dataset final...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "['fecha_compra']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m    Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_17996\\3918065725.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     37\u001b[39m     \u001b[38;5;66;03m# Agregar campo de stock simulado\u001b[39;00m\n\u001b[32m     38\u001b[39m     ventas_limpio[\u001b[33m'cantidad_stock'\u001b[39m] = np.random.randint(\u001b[32m0\u001b[39m, \u001b[32m100\u001b[39m, len(ventas_limpio))\n\u001b[32m     39\u001b[39m \n\u001b[32m     40\u001b[39m     \u001b[38;5;66;03m# Limpiar valores nulos\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     ventas_limpio.dropna(subset=[\u001b[33m'fecha_compra'\u001b[39m, \u001b[33m'precio'\u001b[39m], inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     42\u001b[39m \n\u001b[32m     43\u001b[39m     print(f\"‚úÖ Dataset limpio: {ventas_limpio.shape}\")\n\u001b[32m     44\u001b[39m     print(f\"üìã Columnas finales: {list(ventas_limpio.columns)}\")\n",
      "\u001b[32mD:\\Programs\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, axis, how, thresh, subset, inplace, ignore_index)\u001b[39m\n\u001b[32m   6673\u001b[39m             ax = self._get_axis(agg_axis)\n\u001b[32m   6674\u001b[39m             indices = ax.get_indexer_for(subset)\n\u001b[32m   6675\u001b[39m             check = indices == -\u001b[32m1\u001b[39m\n\u001b[32m   6676\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m check.any():\n\u001b[32m-> \u001b[39m\u001b[32m6677\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m KeyError(np.array(subset)[check].tolist())\n\u001b[32m   6678\u001b[39m             agg_obj = self.take(indices, axis=agg_axis)\n\u001b[32m   6679\u001b[39m \n\u001b[32m   6680\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m thresh \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m lib.no_default:\n",
      "\u001b[31mKeyError\u001b[39m: ['fecha_compra']"
     ]
    }
   ],
   "source": [
    "# Limpiar y preparar el dataset final\n",
    "print(\"\\nüßπ Limpiando dataset final...\")\n",
    "\n",
    "if not ventas_df.empty:\n",
    "    # Seleccionar columnas relevantes y renombrar\n",
    "    columnas_finales = {\n",
    "        'order_id': 'pedido_id',\n",
    "        'order_purchase_date': 'fecha_compra',\n",
    "        'order_status': 'estado_pedido',\n",
    "        'product_id': 'producto_id',\n",
    "        'product_name_lenght': 'longitud_nombre_producto',\n",
    "        'product_description_lenght': 'longitud_descripcion_producto',\n",
    "        'product_photos_qty': 'cantidad_fotos_producto',\n",
    "        'product_weight_g': 'peso_producto_g',\n",
    "        'product_length_cm': 'longitud_producto_cm',\n",
    "        'product_height_cm': 'altura_producto_cm',\n",
    "        'product_width_cm': 'ancho_producto_cm',\n",
    "        'price': 'precio',\n",
    "        'freight_value': 'valor_flete',\n",
    "        'customer_id': 'cliente_id',\n",
    "        'customer_city': 'ciudad_cliente',\n",
    "        'customer_state': 'estado_cliente',\n",
    "        'seller_id': 'vendedor_id',\n",
    "        'seller_city': 'ciudad_vendedor',\n",
    "        'seller_state': 'estado_vendedor'\n",
    "    }\n",
    "    \n",
    "    # Filtrar columnas que existen\n",
    "    columnas_existentes = {k: v for k, v in columnas_finales.items() if k in ventas_df.columns}\n",
    "    ventas_limpio = ventas_df[list(columnas_existentes.keys())].copy()\n",
    "    ventas_limpio.rename(columns=columnas_existentes, inplace=True)\n",
    "    \n",
    "    # Agregar campos calculados\n",
    "    if 'precio' in ventas_limpio.columns and 'valor_flete' in ventas_limpio.columns:\n",
    "        ventas_limpio['precio_total'] = ventas_limpio['precio'] + ventas_limpio['valor_flete']\n",
    "    \n",
    "    # Agregar campo de stock simulado\n",
    "    ventas_limpio['cantidad_stock'] = np.random.randint(0, 100, len(ventas_limpio))\n",
    "    \n",
    "    # Limpiar valores nulos\n",
    "    ventas_limpio.dropna(subset=['fecha_compra', 'precio'], inplace=True)\n",
    "    \n",
    "    print(f\"‚úÖ Dataset limpio: {ventas_limpio.shape}\")\n",
    "    print(f\"üìã Columnas finales: {list(ventas_limpio.columns)}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No se pudo crear el dataset combinado, usando datos de ejemplo\")\n",
    "    # Crear dataset de ejemplo\n",
    "    ventas_limpio = pd.DataFrame({\n",
    "        'pedido_id': range(1, 1001),\n",
    "        'fecha_compra': pd.date_range('2023-01-01', periods=1000, freq='D'),\n",
    "        'producto_id': np.random.randint(1, 101, 1000),\n",
    "        'precio': np.random.uniform(10, 500, 1000),\n",
    "        'cliente_id': np.random.randint(1, 201, 1000),\n",
    "        'ciudad_cliente': np.random.choice(['S√£o Paulo', 'Rio de Janeiro', 'Bras√≠lia', 'Salvador', 'Fortaleza'], 1000),\n",
    "        'cantidad_stock': np.random.randint(0, 100, 1000)\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizaciones del EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaciones del EDA\n",
    "print(\"üìä CREANDO VISUALIZACIONES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Configurar subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('An√°lisis Exploratorio de Datos - Brazilian E-commerce', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Distribuci√≥n de precios\n",
    "if 'precio' in ventas_limpio.columns:\n",
    "    axes[0, 0].hist(ventas_limpio['precio'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0, 0].set_title('Distribuci√≥n de Precios')\n",
    "    axes[0, 0].set_xlabel('Precio (R$)')\n",
    "    axes[0, 0].set_ylabel('Frecuencia')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Ventas por mes\n",
    "if 'fecha_compra' in ventas_limpio.columns:\n",
    "    ventas_por_mes = ventas_limpio.groupby(ventas_limpio['fecha_compra'].dt.to_period('M')).size()\n",
    "    axes[0, 1].plot(range(len(ventas_por_mes)), ventas_por_mes.values, marker='o', linewidth=2, markersize=6)\n",
    "    axes[0, 1].set_title('Ventas por Mes')\n",
    "    axes[0, 1].set_xlabel('Mes')\n",
    "    axes[0, 1].set_ylabel('N√∫mero de Ventas')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Top ciudades por ventas\n",
    "if 'ciudad_cliente' in ventas_limpio.columns:\n",
    "    top_ciudades = ventas_limpio['ciudad_cliente'].value_counts().head(10)\n",
    "    axes[1, 0].barh(range(len(top_ciudades)), top_ciudades.values, color='lightcoral')\n",
    "    axes[1, 0].set_yticks(range(len(top_ciudades)))\n",
    "    axes[1, 0].set_yticklabels(top_ciudades.index)\n",
    "    axes[1, 0].set_title('Top 10 Ciudades por Ventas')\n",
    "    axes[1, 0].set_xlabel('N√∫mero de Ventas')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Distribuci√≥n de stock\n",
    "if 'cantidad_stock' in ventas_limpio.columns:\n",
    "    axes[1, 1].hist(ventas_limpio['cantidad_stock'], bins=20, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "    axes[1, 1].set_title('Distribuci√≥n de Stock')\n",
    "    axes[1, 1].set_xlabel('Cantidad en Stock')\n",
    "    axes[1, 1].set_ylabel('Frecuencia')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualizaciones creadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conexi√≥n y Carga a MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n de MongoDB\n",
    "MONGO_URI = \"mongodb://admin:password123@localhost:27017/\"\n",
    "DB_NAME = \"ventas_tienda_db\"\n",
    "COLLECTION_NAME = \"ventas\"\n",
    "\n",
    "print(\"üîå Conectando a MongoDB...\")\n",
    "\n",
    "try:\n",
    "    client = MongoClient(MONGO_URI)\n",
    "    client.admin.command('ping')\n",
    "    print(\"‚úÖ Conexi√≥n exitosa a MongoDB\")\n",
    "    \n",
    "    db = client[DB_NAME]\n",
    "    collection = db[COLLECTION_NAME]\n",
    "    \n",
    "    # Limpiar colecci√≥n existente\n",
    "    collection.delete_many({})\n",
    "    print(\"üßπ Colecci√≥n limpiada\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error de conexi√≥n: {e}\")\n",
    "    print(\"üí° Aseg√∫rate de que MongoDB est√© ejecut√°ndose con: docker-compose -f docker/docker-compose.yml up -d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos a MongoDB\n",
    "print(\"\\nüì§ Cargando datos a MongoDB...\")\n",
    "\n",
    "try:\n",
    "    # Convertir DataFrame a documentos\n",
    "    records = ventas_limpio.to_dict('records')\n",
    "    \n",
    "    # Procesar fechas\n",
    "    for record in records:\n",
    "        if isinstance(record.get('fecha_compra'), str):\n",
    "            record['fecha_compra'] = pd.to_datetime(record['fecha_compra'])\n",
    "    \n",
    "    # Insertar en lotes para mejor rendimiento\n",
    "    batch_size = 1000\n",
    "    total_inserted = 0\n",
    "    \n",
    "    for i in range(0, len(records), batch_size):\n",
    "        batch = records[i:i + batch_size]\n",
    "        result = collection.insert_many(batch)\n",
    "        total_inserted += len(result.inserted_ids)\n",
    "        print(f\"  üì¶ Lote {i//batch_size + 1}: {len(result.inserted_ids)} registros\")\n",
    "    \n",
    "    print(f\"\\nüéâ Total de registros insertados: {total_inserted}\")\n",
    "    \n",
    "    # Verificar inserci√≥n\n",
    "    count = collection.count_documents({})\n",
    "    print(f\"üìä Documentos en la colecci√≥n: {count}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error al cargar datos: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Verificaci√≥n de Replicaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar replicaci√≥n entre nodos\n",
    "print(\"üîÑ VERIFICANDO REPLICACI√ìN\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # Insertar documento de prueba en el primario\n",
    "    test_doc = {\n",
    "        'pedido_id': 'TEST-001',\n",
    "        'fecha_compra': datetime.now(),\n",
    "        'producto_id': 'PROD-TEST',\n",
    "        'precio': 999.99,\n",
    "        'cliente_id': 99999,\n",
    "        'ciudad_cliente': 'Ciudad de Prueba',\n",
    "        'cantidad_stock': 50,\n",
    "        'test_replicacion': True\n",
    "    }\n",
    "    \n",
    "    result = collection.insert_one(test_doc)\n",
    "    print(f\"‚úÖ Documento de prueba insertado: {result.inserted_id}\")\n",
    "    \n",
    "    # Esperar a que se replique\n",
    "    import time\n",
    "    print(\"‚è≥ Esperando replicaci√≥n...\")\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # Verificar en nodos secundarios\n",
    "    secondary_ports = [27018, 27019]\n",
    "    \n",
    "    for port in secondary_ports:\n",
    "        try:\n",
    "            secondary_uri = f\"mongodb://admin:password123@localhost:{port}/\"\n",
    "            secondary_client = MongoClient(secondary_uri)\n",
    "            secondary_db = secondary_client[DB_NAME]\n",
    "            secondary_collection = secondary_db[COLLECTION_NAME]\n",
    "            \n",
    "            # Buscar documento de prueba\n",
    "            doc = secondary_collection.find_one({'pedido_id': 'TEST-001'})\n",
    "            \n",
    "            if doc:\n",
    "                print(f\"‚úÖ Replicaci√≥n exitosa en puerto {port}: {doc['producto_id']} - ${doc['precio']}\")\n",
    "            else:\n",
    "                print(f\"‚ùå Replicaci√≥n fall√≥ en puerto {port}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è No se pudo verificar puerto {port}: {e}\")\n",
    "    \n",
    "    # Limpiar documento de prueba\n",
    "    collection.delete_one({'pedido_id': 'TEST-001'})\n",
    "    print(\"üßπ Documento de prueba eliminado\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error en verificaci√≥n: {e}\")\n",
    "    print(\"üí° Verifica que el replica set est√© configurado correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Resumen del EDA y ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumen final\n",
    "print(\"üìã RESUMEN DEL PROCESO EDA Y ETL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nüìä DATASET ORIGINAL:\")\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"  - {name}: {df.shape}\")\n",
    "\n",
    "print(f\"\\nüîÑ DATASET PROCESADO:\")\n",
    "print(f\"  - Dimensiones: {ventas_limpio.shape}\")\n",
    "print(f\"  - Columnas: {list(ventas_limpio.columns)}\")\n",
    "\n",
    "print(f\"\\nüìà ESTAD√çSTICAS CLAVE:\")\n",
    "if 'precio' in ventas_limpio.columns:\n",
    "    print(f\"  - Precio promedio: R$ {ventas_limpio['precio'].mean():.2f}\")\n",
    "    print(f\"  - Precio m√°ximo: R$ {ventas_limpio['precio'].max():.2f}\")\n",
    "    print(f\"  - Precio m√≠nimo: R$ {ventas_limpio['precio'].min():.2f}\")\n",
    "\n",
    "if 'fecha_compra' in ventas_limpio.columns:\n",
    "    print(f\"  - Per√≠odo: {ventas_limpio['fecha_compra'].min().date()} a {ventas_limpio['fecha_compra'].max().date()}\")\n",
    "\n",
    "if 'ciudad_cliente' in ventas_limpio.columns:\n",
    "    print(f\"  - Ciudades √∫nicas: {ventas_limpio['ciudad_cliente'].nunique()}\")\n",
    "\n",
    "print(f\"\\nüóÑÔ∏è MONGODB:\")\n",
    "try:\n",
    "    count = collection.count_documents({})\n",
    "    print(f\"  - Documentos cargados: {count}\")\n",
    "    print(f\"  - Base de datos: {DB_NAME}\")\n",
    "    print(f\"  - Colecci√≥n: {COLLECTION_NAME}\")\n",
    "except:\n",
    "    print(\"  - No disponible\")\n",
    "\n",
    "print(f\"\\nüéâ ¬°PROCESO COMPLETADO EXITOSAMENTE!\")\n",
    "print(f\"üí° Ahora puedes ejecutar el notebook de Consultas CRUD para probar las operaciones\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
