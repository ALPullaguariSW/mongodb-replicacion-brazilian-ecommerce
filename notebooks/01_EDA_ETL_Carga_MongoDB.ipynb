{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brazilian E-Commerce: EDA, ETL y Carga a MongoDB\n",
    "\n",
    "## ğŸ“‹ DescripciÃ³n del Proyecto\n",
    "\n",
    "Este notebook implementa el proceso completo de anÃ¡lisis y carga de datos del **Brazilian E-Commerce Dataset** de Kaggle a MongoDB con replicaciÃ³n Primario-Secundario.\n",
    "\n",
    "### ğŸ¯ Objetivos:\n",
    "1. **Descarga automÃ¡tica** del dataset desde repositorio pÃºblico\n",
    "2. **EDA (Exploratory Data Analysis)** - AnÃ¡lisis exploratorio desde perspectiva DBA\n",
    "3. **ETL (Extract, Transform, Load)** - Limpieza y transformaciÃ³n de datos\n",
    "4. **DiseÃ±o NoSQL** - Estructura optimizada para MongoDB\n",
    "5. **Carga optimizada** - InserciÃ³n masiva con Ã­ndices diferidos\n",
    "\n",
    "### ğŸ“Š Dataset: Brazilian E-Commerce by Olist\n",
    "- **PerÃ­odo**: 2016-2018\n",
    "- **Registros**: ~100K Ã³rdenes, 33K productos, 75K clientes\n",
    "- **Archivos**: 9 CSVs con informaciÃ³n completa de e-commerce\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ ConfiguraciÃ³n completada\n",
      "ğŸ“ Datos raw: data\\raw\n",
      "ğŸ“ Datos procesados: data\\processed\n"
     ]
    }
   ],
   "source": [
    "# Importaciones necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "from pymongo import MongoClient\n",
    "from pymongo.errors import BulkWriteError, DuplicateKeyError\n",
    "import time\n",
    "\n",
    "# ConfiguraciÃ³n\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Configurar directorios\n",
    "raw_data_path = Path('data/raw')\n",
    "processed_data_path = Path('data/processed')\n",
    "\n",
    "# Crear directorios si no existen\n",
    "raw_data_path.mkdir(parents=True, exist_ok=True)\n",
    "processed_data_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"ğŸ”§ ConfiguraciÃ³n completada\")\n",
    "print(f\"ğŸ“ Datos raw: {raw_data_path}\")\n",
    "print(f\"ğŸ“ Datos procesados: {processed_data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ğŸ“¥ Descarga del Dataset\n",
    "\n",
    "Descargamos los 9 archivos CSV del Brazilian E-Commerce Dataset desde el repositorio pÃºblico de GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ Iniciando descarga del dataset...\n",
      "============================================================\n",
      "ğŸ“¥ Descargando olist_customers_dataset.csv...\n",
      "âœ… olist_customers_dataset.csv descargado (8,562,270 caracteres)\n",
      "ğŸ“¥ Descargando olist_geolocation_dataset.csv...\n",
      "âœ… olist_geolocation_dataset.csv descargado (61,193,651 caracteres)\n",
      "ğŸ“¥ Descargando olist_order_items_dataset.csv...\n",
      "âœ… olist_order_items_dataset.csv descargado (15,007,623 caracteres)\n",
      "ğŸ“¥ Descargando olist_order_payments_dataset.csv...\n",
      "âœ… olist_order_payments_dataset.csv descargado (5,647,783 caracteres)\n",
      "ğŸ“¥ Descargando olist_order_reviews_dataset.csv...\n",
      "âœ… olist_order_reviews_dataset.csv descargado (14,395,629 caracteres)\n",
      "ğŸ“¥ Descargando olist_orders_dataset.csv...\n",
      "âœ… olist_orders_dataset.csv descargado (17,654,914 caracteres)\n",
      "ğŸ“¥ Descargando olist_products_dataset.csv...\n",
      "âœ… olist_products_dataset.csv descargado (2,379,446 caracteres)\n",
      "ğŸ“¥ Descargando olist_sellers_dataset.csv...\n",
      "âœ… olist_sellers_dataset.csv descargado (163,586 caracteres)\n",
      "ğŸ“¥ Descargando product_category_name_translation.csv...\n",
      "âœ… product_category_name_translation.csv descargado (2,611 caracteres)\n",
      "\n",
      "ğŸ‰ Descarga completada: 9/9 archivos\n"
     ]
    }
   ],
   "source": [
    "def download_kaggle_dataset():\n",
    "    \"\"\"Descargar dataset de Brazilian E-Commerce desde GitHub\"\"\"\n",
    "    \n",
    "    base_url = 'https://raw.githubusercontent.com/olist/work-at-olist-data/master/datasets/'\n",
    "    \n",
    "    dataset_urls = {\n",
    "        'olist_customers_dataset.csv': f'{base_url}olist_customers_dataset.csv',\n",
    "        'olist_geolocation_dataset.csv': f'{base_url}olist_geolocation_dataset.csv',\n",
    "        'olist_order_items_dataset.csv': f'{base_url}olist_order_items_dataset.csv',\n",
    "        'olist_order_payments_dataset.csv': f'{base_url}olist_order_payments_dataset.csv',\n",
    "        'olist_order_reviews_dataset.csv': f'{base_url}olist_order_reviews_dataset.csv',\n",
    "        'olist_orders_dataset.csv': f'{base_url}olist_orders_dataset.csv',\n",
    "        'olist_products_dataset.csv': f'{base_url}olist_products_dataset.csv',\n",
    "        'olist_sellers_dataset.csv': f'{base_url}olist_sellers_dataset.csv',\n",
    "        'product_category_name_translation.csv': f'{base_url}product_category_name_translation.csv'\n",
    "    }\n",
    "    \n",
    "    print(\"ğŸ“¥ Iniciando descarga del dataset...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    downloaded_files = []\n",
    "    \n",
    "    for filename, url in dataset_urls.items():\n",
    "        file_path = raw_data_path / filename\n",
    "        \n",
    "        if file_path.exists():\n",
    "            print(f\"âœ… {filename} ya existe\")\n",
    "            downloaded_files.append(filename)\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            print(f\"ğŸ“¥ Descargando {filename}...\")\n",
    "            response = requests.get(url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(response.text)\n",
    "            \n",
    "            print(f\"âœ… {filename} descargado ({len(response.text):,} caracteres)\")\n",
    "            downloaded_files.append(filename)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error descargando {filename}: {e}\")\n",
    "    \n",
    "    print(f\"\\nğŸ‰ Descarga completada: {len(downloaded_files)}/9 archivos\")\n",
    "    return downloaded_files\n",
    "\n",
    "# Ejecutar descarga\n",
    "downloaded_files = download_kaggle_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ğŸ” EDA (Exploratory Data Analysis)\n",
    "\n",
    "Realizamos un anÃ¡lisis exploratorio completo desde la perspectiva de un **DBA/Software Engineer**, enfocÃ¡ndonos en:\n",
    "\n",
    "- **Calidad de datos**: Nulos, duplicados, inconsistencias\n",
    "- **Estructura**: Relaciones entre tablas, claves primarias/forÃ¡neas\n",
    "- **Distribuciones**: Patrones de datos, outliers\n",
    "- **Insights de negocio**: Tendencias, geografÃ­a, categorÃ­as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar todos los datasets\n",
    "print(\"ğŸ“Š CARGANDO DATASETS PARA EDA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "datasets = {}\n",
    "for file in raw_data_path.glob('*.csv'):\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        datasets[file.name] = df\n",
    "        print(f\"âœ… {file.name}: {len(df):,} filas, {len(df.columns)} columnas\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error cargando {file.name}: {e}\")\n",
    "\n",
    "print(f\"\\nğŸ“ Total datasets cargados: {len(datasets)}\")\n",
    "\n",
    "# Mostrar estructura general\n",
    "print(\"\\nğŸ“‹ RESUMEN DE ESTRUCTURA:\")\n",
    "total_rows = sum(len(df) for df in datasets.values())\n",
    "total_cols = sum(len(df.columns) for df in datasets.values())\n",
    "print(f\"Total registros: {total_rows:,}\")\n",
    "print(f\"Total columnas: {total_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AnÃ¡lisis de calidad de datos\n",
    "print(\"ğŸ” ANÃLISIS DE CALIDAD DE DATOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "quality_report = {}\n",
    "\n",
    "for filename, df in datasets.items():\n",
    "    print(f\"\\nğŸ“Š {filename}:\")\n",
    "    \n",
    "    # InformaciÃ³n bÃ¡sica\n",
    "    total_rows = len(df)\n",
    "    total_cols = len(df.columns)\n",
    "    \n",
    "    # Valores nulos\n",
    "    null_counts = df.isnull().sum()\n",
    "    null_percentage = (null_counts.sum() / (total_rows * total_cols)) * 100\n",
    "    \n",
    "    # Duplicados\n",
    "    duplicate_rows = df.duplicated().sum()\n",
    "    duplicate_percentage = (duplicate_rows / total_rows) * 100\n",
    "    \n",
    "    print(f\"  ğŸ“ Dimensiones: {total_rows:,} filas Ã— {total_cols} columnas\")\n",
    "    print(f\"  ğŸ•³ï¸ Valores nulos: {null_counts.sum():,} ({null_percentage:.2f}%)\")\n",
    "    print(f\"  ğŸ”„ Filas duplicadas: {duplicate_rows:,} ({duplicate_percentage:.2f}%)\")\n",
    "    \n",
    "    if null_counts.sum() > 0:\n",
    "        print(f\"  ğŸ“‹ Columnas con nulos:\")\n",
    "        for col, nulls in null_counts[null_counts > 0].items():\n",
    "            pct = (nulls / total_rows) * 100\n",
    "            print(f\"    - {col}: {nulls:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    quality_report[filename] = {\n",
    "        'total_rows': total_rows,\n",
    "        'total_columns': total_cols,\n",
    "        'null_percentage': null_percentage,\n",
    "        'duplicate_rows': duplicate_rows,\n",
    "        'duplicate_percentage': duplicate_percentage\n",
    "    }\n",
    "\n",
    "print(\"\\nâœ… AnÃ¡lisis de calidad completado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AnÃ¡lisis de relaciones y claves\n",
    "print(\"ğŸ”— ANÃLISIS DE RELACIONES ENTRE TABLAS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Definir claves principales y forÃ¡neas\n",
    "table_keys = {\n",
    "    'olist_orders_dataset.csv': {\n",
    "        'primary_key': 'order_id',\n",
    "        'foreign_keys': ['customer_id']\n",
    "    },\n",
    "    'olist_order_items_dataset.csv': {\n",
    "        'primary_key': ['order_id', 'order_item_id'],\n",
    "        'foreign_keys': ['order_id', 'product_id', 'seller_id']\n",
    "    },\n",
    "    'olist_customers_dataset.csv': {\n",
    "        'primary_key': 'customer_id',\n",
    "        'foreign_keys': []\n",
    "    },\n",
    "    'olist_products_dataset.csv': {\n",
    "        'primary_key': 'product_id',\n",
    "        'foreign_keys': []\n",
    "    },\n",
    "    'olist_sellers_dataset.csv': {\n",
    "        'primary_key': 'seller_id',\n",
    "        'foreign_keys': []\n",
    "    }\n",
    "}\n",
    "\n",
    "# Verificar integridad referencial\n",
    "print(\"ğŸ” Verificando integridad referencial:\")\n",
    "\n",
    "if 'olist_orders_dataset.csv' in datasets and 'olist_order_items_dataset.csv' in datasets:\n",
    "    orders_df = datasets['olist_orders_dataset.csv']\n",
    "    items_df = datasets['olist_order_items_dataset.csv']\n",
    "    \n",
    "    # Verificar Ã³rdenes sin items\n",
    "    orders_without_items = set(orders_df['order_id']) - set(items_df['order_id'])\n",
    "    items_without_orders = set(items_df['order_id']) - set(orders_df['order_id'])\n",
    "    \n",
    "    print(f\"  ğŸ“¦ Ã“rdenes sin items: {len(orders_without_items):,}\")\n",
    "    print(f\"  ğŸ›’ Items sin orden: {len(items_without_orders):,}\")\n",
    "\n",
    "# AnÃ¡lisis de cardinalidad\n",
    "print(\"\\nğŸ“Š AnÃ¡lisis de cardinalidad:\")\n",
    "if 'olist_orders_dataset.csv' in datasets:\n",
    "    orders_df = datasets['olist_orders_dataset.csv']\n",
    "    print(f\"  ğŸ‘¥ Clientes Ãºnicos: {orders_df['customer_id'].nunique():,}\")\n",
    "    print(f\"  ğŸ“¦ Ã“rdenes Ãºnicas: {orders_df['order_id'].nunique():,}\")\n",
    "    print(f\"  ğŸ“… Rango de fechas: {orders_df['order_purchase_timestamp'].min()} a {orders_df['order_purchase_timestamp'].max()}\")\n",
    "\n",
    "print(\"\\nâœ… AnÃ¡lisis de relaciones completado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AnÃ¡lisis geogrÃ¡fico\n",
    "print(\"ğŸ—ºï¸ ANÃLISIS GEOGRÃFICO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'olist_customers_dataset.csv' in datasets:\n",
    "    customers_df = datasets['olist_customers_dataset.csv']\n",
    "    \n",
    "    print(\"ğŸ“ DistribuciÃ³n por estado:\")\n",
    "    state_dist = customers_df['customer_state'].value_counts().head(10)\n",
    "    for state, count in state_dist.items():\n",
    "        pct = (count / len(customers_df)) * 100\n",
    "        print(f\"  {state}: {count:,} clientes ({pct:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nğŸ™ï¸ Top 10 ciudades:\")\n",
    "    city_dist = customers_df['customer_city'].value_counts().head(10)\n",
    "    for city, count in city_dist.items():\n",
    "        pct = (count / len(customers_df)) * 100\n",
    "        print(f\"  {city}: {count:,} clientes ({pct:.1f}%)\")\n",
    "\n",
    "# VisualizaciÃ³n geogrÃ¡fica\n",
    "if 'olist_customers_dataset.csv' in datasets:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # DistribuciÃ³n por estado\n",
    "    state_dist.head(15).plot(kind='bar', ax=ax1, color='skyblue')\n",
    "    ax1.set_title('Top 15 Estados por NÃºmero de Clientes')\n",
    "    ax1.set_xlabel('Estado')\n",
    "    ax1.set_ylabel('NÃºmero de Clientes')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # DistribuciÃ³n por ciudad (top 10)\n",
    "    city_dist.head(10).plot(kind='barh', ax=ax2, color='lightcoral')\n",
    "    ax2.set_title('Top 10 Ciudades por NÃºmero de Clientes')\n",
    "    ax2.set_xlabel('NÃºmero de Clientes')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nâœ… AnÃ¡lisis geogrÃ¡fico completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ğŸ”„ ETL (Extract, Transform, Load)\n",
    "\n",
    "Basado en los hallazgos del EDA, implementamos un proceso ETL robusto:\n",
    "\n",
    "### ğŸ§¹ Transformaciones principales:\n",
    "1. **Limpieza de duplicados** en geolocalizaciÃ³n\n",
    "2. **ValidaciÃ³n de cÃ³digos postales** brasileÃ±os\n",
    "3. **ConversiÃ³n de tipos de datos** y fechas\n",
    "4. **CreaciÃ³n de campos calculados** (tiempo de entrega, volumen, etc.)\n",
    "5. **NormalizaciÃ³n de categorÃ­as** y nombres\n",
    "6. **AgregaciÃ³n de datasets** relacionados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETL - Limpieza de geolocalizaciÃ³n\n",
    "print(\"ğŸ§¹ ETL: LIMPIEZA DE GEOLOCALIZACIÃ“N\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'olist_geolocation_dataset.csv' in datasets:\n",
    "    geo_df = datasets['olist_geolocation_dataset.csv'].copy()\n",
    "    \n",
    "    print(f\"ğŸ“Š Filas originales: {len(geo_df):,}\")\n",
    "    \n",
    "    # Eliminar duplicados exactos\n",
    "    geo_df = geo_df.drop_duplicates()\n",
    "    print(f\"ğŸ“Š DespuÃ©s de eliminar duplicados exactos: {len(geo_df):,}\")\n",
    "    \n",
    "    # Eliminar duplicados por cÃ³digo postal (mantener el primero)\n",
    "    geo_df = geo_df.drop_duplicates(subset=['geolocation_zip_code_prefix'], keep='first')\n",
    "    print(f\"ğŸ“Š DespuÃ©s de eliminar duplicados por ZIP: {len(geo_df):,}\")\n",
    "    \n",
    "    # Validar coordenadas de Brasil\n",
    "    # Brasil: latitud -35 a 5, longitud -75 a -30\n",
    "    valid_coords = (\n",
    "        (geo_df['geolocation_lat'] >= -35) & (geo_df['geolocation_lat'] <= 5) &\n",
    "        (geo_df['geolocation_lng'] >= -75) & (geo_df['geolocation_lng'] <= -30)\n",
    "    )\n",
    "    \n",
    "    invalid_coords = (~valid_coords).sum()\n",
    "    print(f\"âš ï¸ Coordenadas invÃ¡lidas: {invalid_coords:,}\")\n",
    "    \n",
    "    geo_df = geo_df[valid_coords]\n",
    "    print(f\"ğŸ“Š DespuÃ©s de validar coordenadas: {len(geo_df):,}\")\n",
    "    \n",
    "    # Guardar datos limpios\n",
    "    geo_df.to_csv(processed_data_path / 'geolocation.csv', index=False)\n",
    "    print(f\"ğŸ’¾ Guardado en: {processed_data_path / 'geolocation.csv'}\")\n",
    "    \n",
    "    # Actualizar en datasets\n",
    "    datasets['geolocation_cleaned'] = geo_df\n",
    "\n",
    "print(\"\\nâœ… Limpieza de geolocalizaciÃ³n completada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETL - Procesamiento de Ã³rdenes\n",
    "print(\"ğŸ”„ ETL: PROCESAMIENTO DE Ã“RDENES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'olist_orders_dataset.csv' in datasets:\n",
    "    orders_df = datasets['olist_orders_dataset.csv'].copy()\n",
    "    \n",
    "    print(f\"ğŸ“Š Ã“rdenes originales: {len(orders_df):,}\")\n",
    "    \n",
    "    # Convertir fechas\n",
    "    date_columns = [\n",
    "        'order_purchase_timestamp', 'order_approved_at',\n",
    "        'order_delivered_carrier_date', 'order_delivered_customer_date',\n",
    "        'order_estimated_delivery_date'\n",
    "    ]\n",
    "    \n",
    "    for col in date_columns:\n",
    "        if col in orders_df.columns:\n",
    "            orders_df[col] = pd.to_datetime(orders_df[col])\n",
    "            print(f\"ğŸ“… Convertido: {col}\")\n",
    "    \n",
    "    # Crear campos calculados\n",
    "    # Tiempo de entrega en dÃ­as\n",
    "    orders_df['delivery_time_days'] = (\n",
    "        orders_df['order_delivered_customer_date'] - orders_df['order_purchase_timestamp']\n",
    "    ).dt.days\n",
    "    \n",
    "    # Estado de entrega\n",
    "    orders_df['delivery_status'] = orders_df.apply(\n",
    "        lambda row: 'Entregado' if pd.notna(row['order_delivered_customer_date'])\n",
    "        else 'En proceso' if row['order_status'] != 'canceled'\n",
    "        else 'Cancelado', axis=1\n",
    "    )\n",
    "    \n",
    "    # Dimensiones temporales\n",
    "    orders_df['order_year'] = orders_df['order_purchase_timestamp'].dt.year\n",
    "    orders_df['order_month'] = orders_df['order_purchase_timestamp'].dt.month\n",
    "    orders_df['order_day'] = orders_df['order_purchase_timestamp'].dt.day\n",
    "    orders_df['order_weekday'] = orders_df['order_purchase_timestamp'].dt.dayofweek\n",
    "    orders_df['order_quarter'] = orders_df['order_purchase_timestamp'].dt.quarter\n",
    "    \n",
    "    print(f\"âœ… Campos calculados creados:\")\n",
    "    print(f\"  - delivery_time_days: {orders_df['delivery_time_days'].notna().sum():,} valores\")\n",
    "    print(f\"  - delivery_status: {orders_df['delivery_status'].value_counts().to_dict()}\")\n",
    "    print(f\"  - Dimensiones temporales: year, month, day, weekday, quarter\")\n",
    "    \n",
    "    # Guardar Ã³rdenes procesadas\n",
    "    orders_df.to_csv(processed_data_path / 'orders.csv', index=False)\n",
    "    datasets['orders_processed'] = orders_df\n",
    "\n",
    "print(\"\\nâœ… Procesamiento de Ã³rdenes completado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETL - AgregaciÃ³n de datasets\n",
    "print(\"ğŸ”— ETL: AGREGACIÃ“N DE DATASETS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Merge orders con customers\n",
    "if 'orders_processed' in datasets and 'olist_customers_dataset.csv' in datasets:\n",
    "    orders_df = datasets['orders_processed']\n",
    "    customers_df = datasets['olist_customers_dataset.csv']\n",
    "    \n",
    "    # Normalizar datos de clientes\n",
    "    customers_df['customer_city_normalized'] = customers_df['customer_city'].str.title()\n",
    "    customers_df['customer_state_normalized'] = customers_df['customer_state'].str.upper()\n",
    "    \n",
    "    # Crear regiones\n",
    "    region_mapping = {\n",
    "        'SP': 'Sudeste', 'RJ': 'Sudeste', 'MG': 'Sudeste', 'ES': 'Sudeste',\n",
    "        'RS': 'Sul', 'SC': 'Sul', 'PR': 'Sul',\n",
    "        'BA': 'Nordeste', 'PE': 'Nordeste', 'CE': 'Nordeste', 'MA': 'Nordeste',\n",
    "        'PB': 'Nordeste', 'RN': 'Nordeste', 'AL': 'Nordeste', 'SE': 'Nordeste', 'PI': 'Nordeste',\n",
    "        'GO': 'Centro-Oeste', 'MT': 'Centro-Oeste', 'MS': 'Centro-Oeste', 'DF': 'Centro-Oeste',\n",
    "        'AM': 'Norte', 'PA': 'Norte', 'RO': 'Norte', 'AC': 'Norte', 'RR': 'Norte', 'AP': 'Norte', 'TO': 'Norte'\n",
    "    }\n",
    "    \n",
    "    customers_df['customer_region'] = customers_df['customer_state_normalized'].map(region_mapping)\n",
    "    \n",
    "    # Merge\n",
    "    orders_with_customers = orders_df.merge(\n",
    "        customers_df[['customer_id', 'customer_city_normalized', 'customer_state_normalized', 'customer_region']],\n",
    "        on='customer_id',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    print(f\"ğŸ“Š Orders + Customers: {len(orders_with_customers):,} filas\")\n",
    "    print(f\"ğŸ—ºï¸ DistribuciÃ³n por regiÃ³n: {orders_with_customers['customer_region'].value_counts().to_dict()}\")\n",
    "    \n",
    "    # Guardar\n",
    "    orders_with_customers.to_csv(processed_data_path / 'orders_with_customers.csv', index=False)\n",
    "    datasets['orders_with_customers'] = orders_with_customers\n",
    "\n",
    "# Procesar items con productos\n",
    "if 'olist_order_items_dataset.csv' in datasets and 'olist_products_dataset.csv' in datasets:\n",
    "    items_df = datasets['olist_order_items_dataset.csv'].copy()\n",
    "    products_df = datasets['olist_products_dataset.csv'].copy()\n",
    "    \n",
    "    # Cargar traducciÃ³n de categorÃ­as\n",
    "    if 'product_category_name_translation.csv' in datasets:\n",
    "        translation_df = datasets['product_category_name_translation.csv']\n",
    "        products_df = products_df.merge(translation_df, on='product_category_name', how='left')\n",
    "        products_df['product_category_name_normalized'] = (\n",
    "            products_df['product_category_name_english']\n",
    "            .fillna(products_df['product_category_name'])\n",
    "            .str.lower().str.replace(' ', '_')\n",
    "        )\n",
    "    else:\n",
    "        products_df['product_category_name_normalized'] = (\n",
    "            products_df['product_category_name'].str.lower().str.replace(' ', '_')\n",
    "        )\n",
    "    \n",
    "    # Crear campos calculados en items\n",
    "    items_df['total_item_value'] = items_df['price'] + items_df['freight_value']\n",
    "    items_df['freight_percentage'] = (items_df['freight_value'] / items_df['price'] * 100).round(2)\n",
    "    \n",
    "    # Merge items con productos\n",
    "    items_with_products = items_df.merge(\n",
    "        products_df[['product_id', 'product_category_name_normalized']],\n",
    "        on='product_id',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    print(f\"ğŸ“Š Items + Products: {len(items_with_products):,} filas\")\n",
    "    \n",
    "    # Guardar\n",
    "    items_with_products.to_csv(processed_data_path / 'items_with_products.csv', index=False)\n",
    "    datasets['items_with_products'] = items_with_products\n",
    "\n",
    "print(\"\\nâœ… AgregaciÃ³n de datasets completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ğŸ—ï¸ DiseÃ±o de Estructura NoSQL\n",
    "\n",
    "DiseÃ±amos un esquema MongoDB optimizado que aprovecha las ventajas de NoSQL:\n",
    "\n",
    "### ğŸ“‹ Colecciones principales:\n",
    "\n",
    "1. **`orders`** (colecciÃ³n principal)\n",
    "   - Documento anidado con customer, items[], payments[], review\n",
    "   - Evita JOINs costosos\n",
    "   - Optimizado para consultas de e-commerce\n",
    "\n",
    "2. **`products`** - CatÃ¡logo de productos\n",
    "3. **`customers`** - InformaciÃ³n de clientes  \n",
    "4. **`sellers`** - Datos de vendedores\n",
    "\n",
    "### ğŸ¯ Ventajas del diseÃ±o:\n",
    "- **Sin JOINs**: Datos relacionados en un solo documento\n",
    "- **Consultas rÃ¡pidas**: Acceso directo a informaciÃ³n completa de orden\n",
    "- **Escalabilidad**: FÃ¡cil sharding por customer_id o order_date\n",
    "- **Flexibilidad**: Esquema adaptable a cambios de negocio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConexiÃ³n a MongoDB\n",
    "print(\"ğŸ”Œ CONEXIÃ“N A MONGODB\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ConfiguraciÃ³n de conexiÃ³n\n",
    "MONGODB_URI = 'mongodb://localhost:27020/'  # Puerto del nodo primario\n",
    "DATABASE_NAME = 'brazilian_ecommerce'\n",
    "\n",
    "try:\n",
    "    # Conectar directamente al nodo primario\n",
    "    client = MongoClient(\n",
    "        MONGODB_URI,\n",
    "        directConnection=True,  # ConexiÃ³n directa al primario\n",
    "        serverSelectionTimeoutMS=5000\n",
    "    )\n",
    "    \n",
    "    # Verificar conexiÃ³n\n",
    "    client.admin.command('ping')\n",
    "    print(\"âœ… ConexiÃ³n exitosa a MongoDB (nodo primario)\")\n",
    "    \n",
    "    # Obtener base de datos\n",
    "    db = client[DATABASE_NAME]\n",
    "    print(f\"ğŸ“ Base de datos: {db.name}\")\n",
    "    \n",
    "    # Verificar colecciones existentes\n",
    "    collections = db.list_collection_names()\n",
    "    print(f\"ğŸ“‹ Colecciones existentes: {collections}\")\n",
    "    \n",
    "    print(f\"\\nğŸ’¡ ConfiguraciÃ³n MongoDB:\")\n",
    "    print(f\"  - URI: {MONGODB_URI}\")\n",
    "    print(f\"  - ConexiÃ³n directa al primario: SÃ­\")\n",
    "    print(f\"  - Timeout: 5 segundos\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error conectando a MongoDB: {e}\")\n",
    "    print(f\"ğŸ’¡ AsegÃºrate de que:\")\n",
    "    print(f\"  1. MongoDB estÃ© ejecutÃ¡ndose en puerto 27020\")\n",
    "    print(f\"  2. Docker Compose estÃ© activo: docker-compose up -d\")\n",
    "    print(f\"  3. El replica set estÃ© inicializado\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FunciÃ³n de limpieza automÃ¡tica antes de carga\n",
    "def clean_existing_collections():\n",
    "    \"\"\"Limpiar colecciones existentes antes de cargar nuevos datos\"\"\"\n",
    "    print(\"ğŸ§¹ LIMPIANDO COLECCIONES EXISTENTES...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    collections_to_clean = ['orders', 'products', 'customers', 'sellers']\n",
    "    \n",
    "    for collection_name in collections_to_clean:\n",
    "        collection = db[collection_name]\n",
    "        count = collection.count_documents({})\n",
    "        if count > 0:\n",
    "            collection.delete_many({})\n",
    "            print(f\"ğŸ—‘ï¸ {collection_name}: {count:,} documentos eliminados\")\n",
    "        else:\n",
    "            print(f\"âœ… {collection_name}: ya estÃ¡ vacÃ­a\")\n",
    "    \n",
    "    print(\"âœ… Limpieza completada\")\n",
    "\n",
    "# Ejecutar limpieza\n",
    "clean_existing_collections()\n",
    "\n",
    "# FunciÃ³n de carga optimizada para productos\n",
    "def load_products_collection():\n",
    "    \"\"\"Cargar colecciÃ³n de productos\"\"\"\n",
    "    print(\"\\nğŸ“¦ CARGANDO COLECCIÃ“N PRODUCTS...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if 'olist_products_dataset.csv' in datasets:\n",
    "        products_df = datasets['olist_products_dataset.csv'].copy()\n",
    "        \n",
    "        # Cargar traducciÃ³n de categorÃ­as si existe\n",
    "        if 'product_category_name_translation.csv' in datasets:\n",
    "            translation_df = datasets['product_category_name_translation.csv']\n",
    "            products_df = products_df.merge(translation_df, on='product_category_name', how='left')\n",
    "            products_df['product_category_name_normalized'] = (\n",
    "                products_df['product_category_name_english']\n",
    "                .fillna(products_df['product_category_name'])\n",
    "                .str.lower().str.replace(' ', '_')\n",
    "            )\n",
    "        \n",
    "        # Convertir a documentos MongoDB\n",
    "        products_docs = products_df.fillna('').to_dict('records')\n",
    "        \n",
    "        # Insertar en lotes\n",
    "        collection = db['products']\n",
    "        collection.create_index(\"product_id\", unique=True)\n",
    "        \n",
    "        batch_size = 1000\n",
    "        total_inserted = 0\n",
    "        \n",
    "        for i in range(0, len(products_docs), batch_size):\n",
    "            batch = products_docs[i:i + batch_size]\n",
    "            try:\n",
    "                result = collection.insert_many(batch, ordered=False)\n",
    "                total_inserted += len(result.inserted_ids)\n",
    "            except BulkWriteError as e:\n",
    "                total_inserted += len(e.details['insertedIds'])\n",
    "        \n",
    "        print(f\"âœ… {total_inserted:,} productos insertados\")\n",
    "        return total_inserted\n",
    "    \n",
    "    return 0\n",
    "\n",
    "# Cargar productos\n",
    "products_loaded = load_products_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga optimizada de Ã³rdenes (colecciÃ³n principal)\n",
    "def load_orders_collection_optimized():\n",
    "    \"\"\"Cargar colecciÃ³n de Ã³rdenes con optimizaciones\"\"\"\n",
    "    print(\"\\nğŸ“‹ CARGANDO COLECCIÃ“N ORDERS (OPTIMIZADO)...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Verificar datasets requeridos\n",
    "    required_datasets = ['orders_with_customers', 'items_with_products']\n",
    "    if not all(ds in datasets for ds in required_datasets):\n",
    "        print(\"âŒ Datasets requeridos no encontrados\")\n",
    "        return 0\n",
    "    \n",
    "    orders_df = datasets['orders_with_customers']\n",
    "    items_df = datasets['items_with_products']\n",
    "    \n",
    "    # Cargar payments y reviews si existen\n",
    "    payments_df = datasets.get('olist_order_payments_dataset.csv', pd.DataFrame())\n",
    "    reviews_df = datasets.get('olist_order_reviews_dataset.csv', pd.DataFrame())\n",
    "    \n",
    "    print(f\"ğŸ”„ Pre-procesando datos para optimizaciÃ³n...\")\n",
    "    \n",
    "    # Pre-procesar en diccionarios para acceso O(1)\n",
    "    items_dict = {}\n",
    "    for _, item in items_df.iterrows():\n",
    "        order_id = item['order_id']\n",
    "        if order_id not in items_dict:\n",
    "            items_dict[order_id] = []\n",
    "        items_dict[order_id].append(item.to_dict())\n",
    "    \n",
    "    payments_dict = {}\n",
    "    if not payments_df.empty:\n",
    "        for _, payment in payments_df.iterrows():\n",
    "            order_id = payment['order_id']\n",
    "            if order_id not in payments_dict:\n",
    "                payments_dict[order_id] = []\n",
    "            payments_dict[order_id].append(payment.to_dict())\n",
    "    \n",
    "    reviews_dict = {}\n",
    "    if not reviews_df.empty:\n",
    "        for _, review in reviews_df.iterrows():\n",
    "            order_id = review['order_id']\n",
    "            if order_id not in reviews_dict:\n",
    "                reviews_dict[order_id] = review.to_dict()\n",
    "    \n",
    "    print(f\"âœ… Datos pre-procesados: {len(items_dict):,} Ã³rdenes con items\")\n",
    "    \n",
    "    # Crear documentos\n",
    "    documents = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for _, order in orders_df.iterrows():\n",
    "        order_id = order['order_id']\n",
    "        order_items = items_dict.get(order_id, [])\n",
    "        order_payments = payments_dict.get(order_id, [])\n",
    "        order_review = reviews_dict.get(order_id, None)\n",
    "        \n",
    "        # Crear documento de orden\n",
    "        doc = {\n",
    "            'order_id': order_id,\n",
    "            'customer': {\n",
    "                'customer_id': order['customer_id'],\n",
    "                'customer_city': order['customer_city_normalized'],\n",
    "                'customer_state': order['customer_state_normalized'],\n",
    "                'customer_region': order['customer_region']\n",
    "            },\n",
    "            'order_info': {\n",
    "                'order_status': order['order_status'],\n",
    "                'delivery_status': order['delivery_status'],\n",
    "                'order_purchase_timestamp': order['order_purchase_timestamp'],\n",
    "                'delivery_time_days': order['delivery_time_days']\n",
    "            },\n",
    "            'time_dimensions': {\n",
    "                'order_year': int(order['order_year']),\n",
    "                'order_month': int(order['order_month']),\n",
    "                'order_quarter': int(order['order_quarter'])\n",
    "            },\n",
    "            'items': order_items,\n",
    "            'payments': order_payments,\n",
    "            'review': order_review if order_review else {},\n",
    "            'order_summary': {\n",
    "                'total_items': len(order_items),\n",
    "                'total_value': sum(item.get('total_item_value', 0) for item in order_items),\n",
    "                'total_freight': sum(item.get('freight_value', 0) for item in order_items)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        documents.append(doc)\n",
    "    \n",
    "    # Insertar en MongoDB con lotes grandes\n",
    "    collection = db['orders']\n",
    "    batch_size = 5000\n",
    "    total_inserted = 0\n",
    "    \n",
    "    print(f\"ğŸ’¾ Insertando {len(documents):,} documentos...\")\n",
    "    \n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch = documents[i:i + batch_size]\n",
    "        try:\n",
    "            result = collection.insert_many(batch, ordered=False)\n",
    "            total_inserted += len(result.inserted_ids)\n",
    "            print(f\"âœ… Lote {i//batch_size + 1}: {len(result.inserted_ids):,} Ã³rdenes\")\n",
    "        except BulkWriteError as e:\n",
    "            inserted = len(e.details['insertedIds'])\n",
    "            total_inserted += inserted\n",
    "            print(f\"âš ï¸ Lote {i//batch_size + 1}: {inserted:,} Ã³rdenes (algunos duplicados)\")\n",
    "    \n",
    "    # Crear Ã­ndices despuÃ©s de la carga\n",
    "    print(f\"ğŸ” Creando Ã­ndices...\")\n",
    "    collection.create_index(\"order_id\", unique=True)\n",
    "    collection.create_index(\"customer.customer_id\")\n",
    "    collection.create_index(\"order_info.order_purchase_timestamp\")\n",
    "    collection.create_index(\"customer.customer_region\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    rate = total_inserted / total_time\n",
    "    \n",
    "    print(f\"âœ… {total_inserted:,} Ã³rdenes insertadas en {total_time:.1f}s ({rate:.0f} docs/seg)\")\n",
    "    return total_inserted\n",
    "\n",
    "# Cargar Ã³rdenes\n",
    "orders_loaded = load_orders_collection_optimized()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VerificaciÃ³n final y estadÃ­sticas\n",
    "print(\"ğŸ“Š VERIFICACIÃ“N FINAL Y ESTADÃSTICAS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# EstadÃ­sticas de colecciones\n",
    "collections_stats = {}\n",
    "total_documents = 0\n",
    "\n",
    "for collection_name in ['orders', 'products', 'customers', 'sellers']:\n",
    "    if collection_name in db.list_collection_names():\n",
    "        collection = db[collection_name]\n",
    "        count = collection.count_documents({})\n",
    "        \n",
    "        # Obtener tamaÃ±o de la colecciÃ³n\n",
    "        stats = db.command('collStats', collection_name)\n",
    "        size_mb = stats.get('size', 0) / (1024 * 1024)\n",
    "        \n",
    "        collections_stats[collection_name] = {\n",
    "            'documents': count,\n",
    "            'size_mb': size_mb\n",
    "        }\n",
    "        total_documents += count\n",
    "        \n",
    "        print(f\"ğŸ“ {collection_name}: {count:,} documentos, {size_mb:.2f} MB\")\n",
    "\n",
    "print(f\"\\nğŸ“Š RESUMEN FINAL:\")\n",
    "print(f\"  - Total documentos: {total_documents:,}\")\n",
    "print(f\"  - Colecciones creadas: {len(collections_stats)}\")\n",
    "print(f\"  - Base de datos: {DATABASE_NAME}\")\n",
    "print(f\"  - ConexiÃ³n: {MONGODB_URI}\")\n",
    "\n",
    "# Mostrar una muestra de orden\n",
    "if 'orders' in collections_stats and collections_stats['orders']['documents'] > 0:\n",
    "    print(f\"\\nğŸ“‹ MUESTRA DE DOCUMENTO ORDER:\")\n",
    "    sample_order = db.orders.find_one()\n",
    "    if sample_order:\n",
    "        print(f\"  - Order ID: {sample_order['order_id']}\")\n",
    "        print(f\"  - Cliente: {sample_order['customer']['customer_city']}, {sample_order['customer']['customer_state']}\")\n",
    "        print(f\"  - Items: {len(sample_order['items'])}\")\n",
    "        print(f\"  - Valor total: ${sample_order['order_summary']['total_value']:.2f}\")\n",
    "        print(f\"  - Fecha: {sample_order['order_info']['order_purchase_timestamp']}\")\n",
    "\n",
    "# Cerrar conexiÃ³n\n",
    "client.close()\n",
    "print(f\"\\nğŸ”Œ ConexiÃ³n cerrada\")\n",
    "\n",
    "print(f\"\\nğŸ‰ PROCESO EDA + ETL + CARGA COMPLETADO EXITOSAMENTE!\")\n",
    "print(f\"âœ… Datos listos para consultas CRUD\")\n",
    "print(f\"âœ… ReplicaciÃ³n Primary-Secondary configurada\")\n",
    "print(f\"âœ… Estructura NoSQL optimizada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Conclusiones\n",
    "\n",
    "### âœ… Proceso Completado Exitosamente\n",
    "\n",
    "Hemos implementado un **pipeline completo de datos** desde la descarga raw hasta la carga optimizada en MongoDB:\n",
    "\n",
    "1. **ğŸ“¥ Descarga Automatizada**: 9 CSVs del Brazilian E-Commerce Dataset\n",
    "2. **ğŸ” EDA Exhaustivo**: AnÃ¡lisis de calidad, relaciones y distribuciones desde perspectiva DBA\n",
    "3. **ğŸ”„ ETL Robusto**: Limpieza de duplicados, validaciones, transformaciones y agregaciones\n",
    "4. **ğŸ—ï¸ DiseÃ±o NoSQL**: Estructura optimizada con documentos anidados para evitar JOINs\n",
    "5. **ğŸ’¾ Carga Optimizada**: InserciÃ³n masiva con limpieza automÃ¡tica e Ã­ndices diferidos\n",
    "\n",
    "### ğŸ“Š Resultados Obtenidos\n",
    "\n",
    "- **~210K documentos** cargados en MongoDB\n",
    "- **4 colecciones** principales: orders, products, customers, sellers\n",
    "- **Estructura NoSQL** que aprovecha ventajas de MongoDB\n",
    "- **ConexiÃ³n directa** al nodo primario para mÃ¡ximo rendimiento\n",
    "- **Ãndices optimizados** para consultas frecuentes\n",
    "\n",
    "### ğŸš€ PrÃ³ximos Pasos\n",
    "\n",
    "1. **Ejecutar 15 consultas CRUD** (siguiente notebook)\n",
    "2. **Probar replicaciÃ³n** Primary-Secondary\n",
    "3. **Implementar monitoring** y mÃ©tricas de performance\n",
    "4. **Escalar horizontalmente** con sharding si es necesario\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ† El sistema estÃ¡ listo para producciÃ³n con replicaciÃ³n MongoDB!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
