#!/usr/bin/env python3
"""
An√°lisis Exploratorio de Datos (EDA) - Perspectiva de DBA
Dataset: Brazilian E-Commerce Public Dataset by Olist
"""

import pandas as pd
import numpy as np
from pathlib import Path
import json
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

class EDAAnalyzer:
    def __init__(self, data_path='data/raw'):
        self.data_path = Path(data_path)
        self.datasets = {}
        self.analysis_results = {}
        
    def load_all_datasets(self):
        """Cargar todos los datasets CSV"""
        print("üìä CARGANDO DATASETS...")
        print("="*60)
        
        csv_files = list(self.data_path.glob('*.csv'))
        
        for file_path in csv_files:
            filename = file_path.name
            print(f"üì• Cargando {filename}...")
            
            try:
                # Leer con manejo de errores
                df = pd.read_csv(file_path, low_memory=False)
                self.datasets[filename] = df
                print(f"‚úÖ {filename}: {len(df):,} filas, {len(df.columns)} columnas")
                
            except Exception as e:
                print(f"‚ùå Error cargando {filename}: {e}")
        
        print(f"\nüìÅ Total datasets cargados: {len(self.datasets)}")
        
    def analyze_dataset_structure(self):
        """An√°lisis de estructura de cada dataset"""
        print("\nüîç AN√ÅLISIS DE ESTRUCTURA DE DATASETS")
        print("="*60)
        
        for filename, df in self.datasets.items():
            print(f"\nüìã {filename}")
            print("-" * 40)
            
            # Informaci√≥n b√°sica
            print(f"üìä Filas: {len(df):,}")
            print(f"üìä Columnas: {len(df.columns)}")
            print(f"üìä Memoria: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
            
            # Tipos de datos
            print(f"\nüìù Tipos de datos:")
            for col, dtype in df.dtypes.items():
                print(f"  ‚Ä¢ {col}: {dtype}")
            
            # Valores √∫nicos por columna
            print(f"\nüîë Valores √∫nicos por columna:")
            for col in df.columns:
                unique_count = df[col].nunique()
                print(f"  ‚Ä¢ {col}: {unique_count:,} √∫nicos")
                
            # Valores nulos
            null_counts = df.isnull().sum()
            if null_counts.sum() > 0:
                print(f"\n‚ö†Ô∏è Valores nulos:")
                for col, null_count in null_counts[null_counts > 0].items():
                    percentage = (null_count / len(df)) * 100
                    print(f"  ‚Ä¢ {col}: {null_count:,} ({percentage:.2f}%)")
            else:
                print(f"\n‚úÖ Sin valores nulos")
    
    def analyze_data_quality(self):
        """An√°lisis de calidad de datos"""
        print("\nüîç AN√ÅLISIS DE CALIDAD DE DATOS")
        print("="*60)
        
        quality_report = {}
        
        for filename, df in self.datasets.items():
            print(f"\nüìã {filename}")
            print("-" * 40)
            
            report = {
                'total_rows': len(df),
                'total_columns': len(df.columns),
                'null_percentage': (df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100,
                'duplicate_rows': df.duplicated().sum(),
                'duplicate_percentage': (df.duplicated().sum() / len(df)) * 100
            }
            
            quality_report[filename] = report
            
            print(f"üìä Filas totales: {report['total_rows']:,}")
            print(f"üìä Porcentaje de nulos: {report['null_percentage']:.2f}%")
            print(f"üìä Filas duplicadas: {report['duplicate_rows']:,} ({report['duplicate_percentage']:.2f}%)")
            
            # An√°lisis de columnas espec√≠ficas
            for col in df.columns:
                if df[col].dtype == 'object':
                    # Para columnas de texto
                    empty_strings = (df[col] == '').sum()
                    if empty_strings > 0:
                        print(f"  ‚ö†Ô∏è {col}: {empty_strings:,} strings vac√≠os")
        
        self.analysis_results['quality_report'] = quality_report
    
    def analyze_relationships(self):
        """An√°lisis de relaciones entre datasets"""
        print("\nüîó AN√ÅLISIS DE RELACIONES ENTRE DATASETS")
        print("="*60)
        
        # Identificar claves primarias y for√°neas
        relationships = {
            'customers': {
                'primary_key': 'customer_id',
                'foreign_keys': ['customer_unique_id']
            },
            'orders': {
                'primary_key': 'order_id',
                'foreign_keys': ['customer_id']
            },
            'order_items': {
                'primary_key': ['order_id', 'order_item_id'],
                'foreign_keys': ['order_id', 'product_id', 'seller_id']
            },
            'products': {
                'primary_key': 'product_id',
                'foreign_keys': ['product_category_name']
            },
            'sellers': {
                'primary_key': 'seller_id',
                'foreign_keys': []
            },
            'payments': {
                'primary_key': ['order_id', 'payment_sequential'],
                'foreign_keys': ['order_id']
            },
            'reviews': {
                'primary_key': 'review_id',
                'foreign_keys': ['order_id']
            }
        }
        
        print("üîë CLAVES PRIMARIAS Y FOR√ÅNEAS:")
        for dataset_name, keys in relationships.items():
            print(f"\nüìã {dataset_name}:")
            print(f"  üîë PK: {keys['primary_key']}")
            print(f"  üîó FK: {keys['foreign_keys']}")
        
        # Verificar integridad referencial
        print(f"\nüîç VERIFICACI√ìN DE INTEGRIDAD REFERENCIAL:")
        
        # Verificar customer_id en orders vs customers
        if 'olist_customers_dataset.csv' in self.datasets and 'olist_orders_dataset.csv' in self.datasets:
            customers_df = self.datasets['olist_customers_dataset.csv']
            orders_df = self.datasets['olist_orders_dataset.csv']
            
            customers_ids = set(customers_df['customer_id'])
            orders_customer_ids = set(orders_df['customer_id'])
            
            missing_customers = orders_customer_ids - customers_ids
            orphan_customers = customers_ids - orders_customer_ids
            
            print(f"\nüìä Relaci√≥n Orders-Customers:")
            print(f"  ‚Ä¢ Customer IDs en orders: {len(orders_customer_ids):,}")
            print(f"  ‚Ä¢ Customer IDs en customers: {len(customers_ids):,}")
            print(f"  ‚Ä¢ Orders sin customer: {len(missing_customers):,}")
            print(f"  ‚Ä¢ Customers sin orders: {len(orphan_customers):,}")
    
    def analyze_business_insights(self):
        """An√°lisis de insights de negocio"""
        print("\nüíº AN√ÅLISIS DE INSIGHTS DE NEGOCIO")
        print("="*60)
        
        # An√°lisis temporal
        if 'olist_orders_dataset.csv' in self.datasets:
            orders_df = self.datasets['olist_orders_dataset.csv']
            
            # Convertir fechas
            orders_df['order_purchase_timestamp'] = pd.to_datetime(orders_df['order_purchase_timestamp'])
            
            print(f"\nüìÖ AN√ÅLISIS TEMPORAL:")
            print(f"  ‚Ä¢ Per√≠odo: {orders_df['order_purchase_timestamp'].min()} a {orders_df['order_purchase_timestamp'].max()}")
            print(f"  ‚Ä¢ Total d√≠as: {(orders_df['order_purchase_timestamp'].max() - orders_df['order_purchase_timestamp'].min()).days}")
            
            # An√°lisis por estado
            print(f"\nüó∫Ô∏è AN√ÅLISIS GEOGR√ÅFICO:")
            if 'olist_customers_dataset.csv' in self.datasets:
                customers_df = self.datasets['olist_customers_dataset.csv']
                state_counts = customers_df['customer_state'].value_counts()
                print(f"  ‚Ä¢ Estados con m√°s clientes:")
                for state, count in state_counts.head(5).items():
                    print(f"    - {state}: {count:,} clientes")
        
        # An√°lisis de productos
        if 'olist_products_dataset.csv' in self.datasets:
            products_df = self.datasets['olist_products_dataset.csv']
            
            print(f"\nüì¶ AN√ÅLISIS DE PRODUCTOS:")
            print(f"  ‚Ä¢ Total productos: {len(products_df):,}")
            print(f"  ‚Ä¢ Categor√≠as √∫nicas: {products_df['product_category_name'].nunique():,}")
            
            # Categor√≠as m√°s populares
            category_counts = products_df['product_category_name'].value_counts()
            print(f"  ‚Ä¢ Top 5 categor√≠as:")
            for category, count in category_counts.head(5).items():
                print(f"    - {category}: {count:,} productos")
    
    def generate_recommendations(self):
        """Generar recomendaciones para el ETL"""
        print("\nüí° RECOMENDACIONES PARA ETL")
        print("="*60)
        
        recommendations = []
        
        # Recomendaciones de limpieza
        print("üßπ LIMPIEZA DE DATOS:")
        print("  ‚Ä¢ Eliminar filas duplicadas")
        print("  ‚Ä¢ Manejar valores nulos seg√∫n el contexto")
        print("  ‚Ä¢ Normalizar formatos de fecha")
        print("  ‚Ä¢ Validar c√≥digos postales")
        
        # Recomendaciones de transformaci√≥n
        print("\nüîÑ TRANSFORMACIONES:")
        print("  ‚Ä¢ Crear √≠ndices para claves primarias y for√°neas")
        print("  ‚Ä¢ Agregar campos calculados (total_ventas, etc.)")
        print("  ‚Ä¢ Normalizar categor√≠as de productos")
        print("  ‚Ä¢ Crear dimensiones de tiempo")
        
        # Recomendaciones de MongoDB
        print("\nüóÑÔ∏è ESTRUCTURA MONGODB:")
        print("  ‚Ä¢ Colecci√≥n principal: orders (con embedded items)")
        print("  ‚Ä¢ Colecciones separadas: customers, products, sellers")
        print("  ‚Ä¢ √çndices en campos de consulta frecuente")
        print("  ‚Ä¢ Considerar agregaciones pre-calculadas")
        
        recommendations.extend([
            "Eliminar duplicados antes del procesamiento",
            "Crear √≠ndices en MongoDB para optimizar consultas",
            "Implementar validaci√≥n de datos en el ETL",
            "Considerar particionamiento por fecha",
            "Agregar campos de auditor√≠a (created_at, updated_at)"
        ])
        
        self.analysis_results['recommendations'] = recommendations
    
    def save_analysis_report(self):
        """Guardar reporte de an√°lisis"""
        report_path = Path('data/processed/eda_report.json')
        report_path.parent.mkdir(parents=True, exist_ok=True)
        
        # Preparar datos para JSON
        quality_report_clean = {}
        for filename, report in self.analysis_results.get('quality_report', {}).items():
            quality_report_clean[filename] = {
                'total_rows': int(report['total_rows']),
                'total_columns': int(report['total_columns']),
                'null_percentage': float(report['null_percentage']),
                'duplicate_rows': int(report['duplicate_rows']),
                'duplicate_percentage': float(report['duplicate_percentage'])
            }
        
        json_data = {
            'analysis_date': datetime.now().isoformat(),
            'datasets_analyzed': list(self.datasets.keys()),
            'quality_report': quality_report_clean,
            'recommendations': self.analysis_results.get('recommendations', [])
        }
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nüíæ Reporte guardado en: {report_path}")
    
    def run_full_analysis(self):
        """Ejecutar an√°lisis completo"""
        print("üéØ AN√ÅLISIS EXPLORATORIO DE DATOS - PERSPECTIVA DBA")
        print("="*80)
        
        # Cargar datos
        self.load_all_datasets()
        
        # An√°lisis de estructura
        self.analyze_dataset_structure()
        
        # An√°lisis de calidad
        self.analyze_data_quality()
        
        # An√°lisis de relaciones
        self.analyze_relationships()
        
        # An√°lisis de negocio
        self.analyze_business_insights()
        
        # Recomendaciones
        self.generate_recommendations()
        
        # Guardar reporte
        self.save_analysis_report()
        
        print("\n‚úÖ AN√ÅLISIS COMPLETADO")
        print("üìù Pr√≥ximos pasos:")
        print("   1. Revisar el reporte de calidad")
        print("   2. Implementar el proceso ETL")
        print("   3. Dise√±ar la estructura de MongoDB")

if __name__ == "__main__":
    analyzer = EDAAnalyzer()
    analyzer.run_full_analysis()
